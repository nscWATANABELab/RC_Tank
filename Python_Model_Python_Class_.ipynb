{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nscWATANABELab/RC_Tank/blob/main/Python_Model_Python_Class_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 必要なライブラリをインポートします"
      ],
      "metadata": {
        "id": "zVEYc1LhhMz3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NTGwQTe2NW04"
      },
      "outputs": [],
      "source": [
        "# In[1]:\n",
        "from datetime import datetime\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import time\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout\n",
        "\n",
        "\n",
        "# import os\n",
        "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "logdir = \"=[=\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# データセットを作る"
      ],
      "metadata": {
        "id": "y5XQNq1hipwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYJMUFUHNTfB"
      },
      "outputs": [],
      "source": [
        "DATADIR = \"drive/MyDrive/tank-data/\"\n",
        "CATEGORIES = [\"0\",\"1\",\"2\",\"3\"]\n",
        "IMG_SIZE = 100\n",
        "training_data = []\n",
        "# In[4]:\n",
        "def create_training_data():\n",
        "    for class_num,category in enumerate(CATEGORIES):\n",
        "        path = os.path.join(DATADIR,category)\n",
        "        for image_name in os.listdir(path):\n",
        "            try:\n",
        "                img_array =cv2.imread(os.path.join(path,image_name),)\n",
        "                img_resize_array = cv2.resize(img_array,(200,200))\n",
        "                training_data.append([img_resize_array,class_num])\n",
        "            except Exception as e:\n",
        "                print('error')\n",
        "                pass\n",
        "create_training_data()\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "for feature,label in training_data:\n",
        "    X.append(feature)\n",
        "    y.append(label)\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "print(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#学習する"
      ],
      "metadata": {
        "id": "_lOGuumlivFw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nn_lagHHLgvq"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "#from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "def get_model():\n",
        "    base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(200, 200, 3))\n",
        "    for layers in base_model.layers:\n",
        "        layers.trainable = False\n",
        "\n",
        "    # model = Flatten()(base_model.output)\n",
        "    # model = Dense(128, activation='relu')(model)\n",
        "    # model = Dropout(0.5)(model)\n",
        "    # model = Dense(5, activation='softmax')(model)\n",
        "\n",
        "    model = Flatten()(base_model.output)\n",
        "    model = Dense(128, activation='relu')(model)\n",
        "    model = Dropout(0.5)(model)\n",
        "    model = Dense(4, activation='softmax')(model)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=model)\n",
        "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#     model.fit(X_train, y_train, batch_size=128, epochs=50)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "model = get_model()\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "y_test = np.eye(4)[y]\n",
        "y_train = np.eye(4)[y_train]\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, batch_size=100,validation_split=0.5, epochs=10,callbacks=[tensorboard_callback])\n",
        "model.save('model-wataver1.h5')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1txUO_6x_kg8tb7lL8gTVYRalrc8EqgYi",
      "authorship_tag": "ABX9TyMgE1XhFvsiyJF8cgLT3P6E",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}